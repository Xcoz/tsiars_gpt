Киселева А.С. 

Козырев С.В. 

Научно-исследовательская работа на тему 

 «Выбор процессорной архитектуры для обучения нейронных се-

1.Введение 

тей» 

Вычислительные системы прошли долгий путь с тех пор, как в середине 

XX  века  были  разработаны  первые  электронные  компьютеры.  За  последнее 

десятилетие  технологический  прогресс  привел к  значительному повышению 

вычислительной  мощности,  скорости  и  эффективности  решения  задач  обра-

ботки данных. Двумя ключевыми компонентами современных вычислитель-

ных систем являются процессоры и видеокарты, которые за прошедшие годы 

претерпели впечатляющее развитие. 

 Стоит  понимать,  что  архитектуры  процессора  и  видеокарты  значи-

тельно отличаются друг от друга, в связи с этим их сфера применения до опре-

деленного  времени  была  весьма узконаправленной:  процессоры  (CPU)  отве-

чали за выполнение последовательных операций и вычислений, в то время как 

видеокарты (GPU) отвечали за обработку 3D-графики. С течением времени ар-

хитектуры  CPU  совершенствовались,  но  достигли  определенного  потолка 

своих  возможностей,  в  то  время  как  архитектура  видеокарт  (GPU)  стреми-

тельно наращивала свои обороты, и в последнее время стала применима  для 

параллельной обработки больших объемов данных, и, как следствие, для за-

дач, требующих огромной вычислительной мощности, таких как игры, ренде-

ринг видео и машинное обучение.  

Развитие подобных архитектур сыграло ключевую роль в эволюции вы-

числительных  систем,  предоставляя  возможность  производить  все  более 

сложные вычисления, но вместе с этим появилась потребность в выборе кон-

кретных вычислительных мощностей, которые необходимо использовать для 

эффективного решения поставленных задач. 

 

2.Развитие микропроцессорных систем 

Микропроцессор — это небольшой компьютер на микросхеме, который 

может выполнять вычисления и другие операции. Микропроцессоры проекти-

руются и разрабатываются специализированными группами инженеров и уче-

ных, работающих в таких компаниях, как Intel, AMD, ARM и Qualcomm. 

Первый микропроцессор Intel 4004 был выпущен в 1971 году. Он имел 

4-битную архитектуру и был способен выполнять до 60 000 инструкций в се-

кунду.  С  появлением  микропроцессоров  значительно  уменьшились  и  габа-

риты вычислительных систем, так как за определенный набор инструкций те-

перь отвечает компактный микрочип. 

В течение следующих нескольких лет микропроцессоры продолжали со-

вершенствоваться.  Появились  8-разрядные  Intel  8080  и  16-разрядные  Intel 

8086.  Разрядность  процессора  определяла  размер  обработки  данных  за  один 

такт. Такой параметр, как увеличение разрядности процессора, способствовал 

развитию персональных компьютеров, которые произвели революцию в спо-

собах работы, развлечений и общения людей. 

Одним из наиболее значительных достижений в архитектуре микропро-

цессоров  стало  появление  в  1980-х  годах  архитектуры  CISC  (Complex 

Instruction Set Computing). Процессоры CISC были разработаны для выполне-

ния сложных операций, что сделало их более универсальными, чем предыду-

щие архитектуры. Однако, процессоры с подобной архитектурой отличались 

медленной скоростью обработки и своей дороговизной. 

В ответ на это в 1980-х годах была представлена новая архитектура под 

названием RISC (Reduced Instruction Set Computing). Процессоры RISC были 

разработаны для выполнения более простых операций, что делало их быстрее 

и эффективнее, чем процессоры CISC. Они также были дешевле в производ-

стве, что делало их более доступными для широкого спектра применений. 

В  1990-х  годах  была  представлена  новая  архитектура  под  названием 

MISC  (Minimal  Instruction  Set  Computing).  Процессоры  MISC  были  разрабо-

таны для очень быстрого выполнения небольшого количества операций, что 

делало их идеальными для приложений, требующих быстрой обработки, но не 

требующих универсальности процессоров CISC или RISC. 

Еще  одним  значительным  достижением  в  области  микропроцессоров 

стало появление в 1980-х годах архитектуры ARM (Advanced RISC Machine). 

Процессоры ARM были разработаны с низким энергопотреблением, что сде-

лало  их подходящими  для  использования  в портативных  устройствах, таких 

как смартфоны и планшеты. Сегодня процессоры ARM используются в подав-

ляющем большинстве смартфонов и других портативных устройствах. 

Помимо микропроцессоров, еще одним ключевым событием в области 

компьютерных технологий стало появление в 1990-х годах графических про-

цессоров (GPU). Графические процессоры были разработаны для выполнения 

сложных параллельных вычислений, необходимых для рендеринга высокока-

чественной  графики  в  видеоиграх  и  других  приложениях.  Сегодня  графиче-

ские процессоры используются не только для игр, но и для научных симуля-

ций, машинного обучения и других приложений, интенсивно использующих 

данные. 

 

3. Проблемы развития микропроцессоров 

По мере того, как процессоры становились меньше и имели более плот-

ное расположение транзисторов, их стало труднее охлаждать, что могло при-

вести  к  перегреву  и  снижению  производительности.  Кроме  того,  поскольку 

размер  транзисторов  уменьшился,  количество  энергии,  необходимой  для  их 

работы, увеличилось, что приводило к накоплению тепла и ограничению про-

изводительности. 

Существуют также фундаментальные физические ограничения размера 

транзисторов и скорости, с которой они могут переключаться. Эти ограниче-

ния  наложены  квантовой  механикой  и  тем  фактом,  что  на  атомном  уровне 

электроны ведут себя непредсказуемо и трудно поддаются контролю. Эти пре-

делы называются «концом закона Мура», который заключается в том, что ко-

личество транзисторов в микрочипе удваивается примерно каждые два года, 

но это не может продолжаться бесконечно. 

В результате этих физических ограничений ученые и инженеры изучают 

новые  способы  дальнейшего  повышения  производительности,  например,  за 

счет использования новых материалов, новых архитектур и специализирован-

ных процессоров для конкретных задач. Одной из таких архитектур, как раз и 

стала архитектура графических процессоров, производительность которой, за 

последние годы показала колоссальный прирост. На рисунке 1 демонстриру-

ется производительность GPU в сравнении с CPU. Исходя из этого можно сде-

лать вывод о дальнейших перспективах развития данной архитектуры 

 

 

 

4.CPU 

Рисунок 1 - сравнение производительностей CPU и GPU 

 

Далее  мы более  детально изучим ранее  упомянутые  архитектуры, рас-

смотрим их плюсы и минусы, а также проведем эксперимент, в котором срав-

ним эффективность использования графического и центрального процессоров 

при работе с нейронными сетями. 

Центральный процессор — это микропроцессор, который выполняет ин-

струкции, заданные программой на основе арифметических операций, опера-

ций управления, логики и ввода-вывода. Сегодня во многих компьютерах ис-

пользуется многоядерный процессор, который представляет собой один чип, 

содержащий два или более процессора (ядра). Процессоры с несколькими яд-

рами могут запускать несколько процессов параллельно, таким образом зна-

чительно ускоряя время выполнения. 

 

4.1 Архитектура 

Структура архитектуры ЦП состоит из нескольких компонентов, кото-

рые работают вместе для обработки данных и выполнения вычислений: 

Блок управления (CU) отвечает за выборку и декодирование инструк-

ций из памяти, а также за управление потоком данных между различными ком-

понентами процессора. CU также управляет планированием и распределением 

ресурсов между различными процессами и потоками. 

Арифметико-логическое  устройство  (ALU)  выполняет  арифметиче-

ские и логические операции, такие как сложение, вычитание, умножение и де-

ление. Оно также может выполнять логические операции, такие как И, ИЛИ и 

Регистры – это небольшие, быстродействующие ячейки памяти в про-

цессоре, которые используются для хранения временных данных во время вы-

НЕ. 

числений.  

Кэш – это небольшая высокоскоростная память, в которой хранятся ча-

сто используемые данные. Он используется для уменьшения задержки при об-

ращении к памяти и повышения производительности процессора. 

Блок  управления  памятью  (MMU)  управляет  иерархией  памяти  и 

обеспечивает эффективное хранение программ и данных и доступ к ним. Он 

переводит виртуальные адреса в физические, управляет защитой памяти и кон-

тролирует доступ к памяти. 

• 

• 

• 

• 

• 

  

• 

• 

Интерфейс соединяет ЦП с материнской платой компьютера и обеспе-

чивает необходимую скорость передачи данных для высокоскоростной связи 

между ЦП и другими компонентами. 

Вот основные характеристики процессора: 

Несколько ядер, интегрированых в один чип; 

Способность взаимодействовать с другими компонентами и регу-

лировать распределение ресурсов; 

Использование небольших объемов встроенной памяти; 

Измерение  скорости  в  герцах  (частоту  внутренних  тактовых  им-

пульсов процессора в циклах в секунду); 

Измерение эффективности в IPC (количество операций за цикл). 

Графический  процессор  (GPU)  -  это  альтернативная  архитектура  про-

цессора,  основной  задачей  которого  является  отображение  и  обработка  гра-

фики на электронном устройстве. Отчасти графический процессор похож на 

обычный, но внутреннее устройство архитектуры в нем кардинально отлича-

5.GPU 

ется. 

 Графические процессоры в основном бывают двух типов: 

Интегрированные  графические  процессоры,  встроенные  в  мате-

ринскую плату компьютера; 

Дискретные графические процессоры - это отдельный компонент, 

имеющий собственную плату, чип и память. Они больше и более энергозави-

симы, чем интегрированные графические процессоры. 

GeForce 256 от NVIDIA был первым широко используемым GPU, специ-

ально разработанным для графики реального времени, приложений, требую-

щих большого количества арифметических операций и высокой пропускной 

способности памяти.  

 

5.1 Архитектура 

Структура архитектуры GPU состоит из нескольких компонентов, кото-

рые работают вместе для обработки данных и выполнения вычислений: 

Потоковые  мультипроцессоры  (SM)  -  это  фундаментальный  строи-

тельный  блок  архитектуры  GPU. Каждый  SM  содержит  набор  вычислитель-

ных блоков, называемых ядрами CUDA, которые могут выполнять тысячи вы-

числений  одновременно. SM также  имеет  собственный  кэш инструкций, об-

щую память и регистры. 

Ядра CUDA - это вычислительные блоки внутри SM. Эти ядра предна-

значены для параллельного выполнения множества вычислений, что делает их 

идеальными для задач глубокого обучения, требующих большого количества 

матричных операций и сверток.  

Иерархия памяти состоит из нескольких уровней кэшей, общей памяти 

и глобальной памяти. Кэши используются для хранения часто используемых 

данных, а общая память используется для обмена данными между потоками в 

рамках одного блока. Глобальная память используется для хранения данных, 

доступных всем потокам в GPU. 

Блок управления отвечает за управление потоком данных и операций 

между различными компонентами архитектуры GPU.  

Интерфейс соединяет GPU с материнской платой компьютера и обеспе-

чивает необходимую скорость передачи данных для высокоскоростной связи 

между CPU и GPU. 

Вот основные характеристики графического процессора: 

• 

Большое  количество  арифметических  логических  блоков  (ALU) 

способных  обрабатывать  огромное  количество  данных  в несколько  потоков. 

Кроме  того,  графические  процессоры  включают  сотни  ядер,  которые  могут 

управлять несколькими потоками обработки одновременно. 

• 

Подключение  через  порты  для  подключения  дискретного  GPU  к 

монитору можно использовать несколько портов. 

• 

Способность  выполнять  векторные  вычисления  и  вычисления  с 

плавающей  точкой,  то  есть  математические  операции  над  приближенным 

представлением действительных чисел.  

• 

Пригодность для параллельных вычислений 

6.Сравнение CPU и GPU при обучении нейросетей 

6.1 Экспериментальная платформа 

Эксперименты проводились на компьютере под управлением Windows 

10 Professional x64 Custom Version: 

> TurboBoost Active 4.3GHz AllCore 

1.Процессор - Intel(R) Core(TM) i5-10400F CPU @ 2.90GHz   2.90 GHz -

2.Видеокарта - MSI NVIDIA GeForce RTX 3070 Gaming Z Trio GDDR6 

3.Материнская плата - ASRock B460M Pro4 

4.Оперативная память - HyperX Fury GDDR4 Dual Channel 16gb 

6.2 Экспериментальное обоснование выбора GPU 

Основным преимуществом использования графического процессора для 

обучения нейронных сетей, как мы уже выяснили ранее, является его способ-

ность выполнять параллельную обработку данных. В отличие от центральных 

процессоров, которые обычно имеют несколько ядер, оптимизированных для 

последовательной обработки, графические процессоры имеют сотни или даже 

тысячи  ядер  меньшего  размера,  оптимизированных  для  параллельной  обра-

ботки. Это  позволяет выполнять множество  вычислений одновременно, зна-

чительно сокращая время, необходимое для обучения нейронной сети. 

8Gb 

 

 

 

Рисунок 2 - Архитектура ЦП (слева) и архитектура ГП (справа) 

 

Рисунок 3 - Иллюстрация блоков и потоков на GPU 

 

 

В  дополнение  к  возможностям  параллельной  обработки  графических 

процессоров  они  также  имеют  высокую  пропускную  способность  памяти. 

Пропускная способность памяти относится к скорости, с которой данные мо-

гут передаваться между памятью графического процессора и его вычислитель-

ными ядрами. Это имеет решающее значение для обучения нейронных сетей, 

поскольку включает передачу больших объемов данных между слоями сети во 

время каждой итерации процесса обучения. 

 Среди  нейросетевых  моделей,  которые  демонстрируют  наибольшую 

пользу от использования GPU: конволюционные (сверточные) нейронные сети 

(CNN),  рекуррентные  нейронные  сети  (RNN)  и  генеративные  адверсарные 

сети (GAN). Кроме того, многие популярные фреймворки глубокого обучения, 

такие  как  TensorFlow  и  PyTorch,  имеют  встроенную  поддержку  ускорения 

GPU. Это позволяет легко обучать нейронные сети на графических процессо-

рах, не требуя значительных изменений в коде. 

6.3 Выбор модели нейронной сети 

Для  проведения  эксперимента  мы  будем  использовать  свёрточную 

нейронную  сеть  (CNN)  для  классификации  изображений  из  набора  данных 

MNIST.  Основные  оптимизации  использования  графического  процессора 

включают использование слоев Conv2D и MaxPooling2D, которые можно эф-

фективно распараллелить на графическом процессоре, а также использование 

метода подгонки для пакетного обучения модели. 

 

 

 

 

]) 

 

Листинг 1 - код  

import tensorflow as tf 

import time 

# Архитектура модели 

model = tf.keras.Sequential([ 

    tf.keras.layers.MaxPooling2D((2, 2)), 

    tf.keras.layers.Flatten(), 

    tf.keras.layers.Dense(64, activation='relu'), 

    tf.keras.layers.Dense(10, activation='softmax') 

# Компиляция модели 

model.compile(optimizer='adam', 

              loss='sparse_categorical_crossentropy', 

              metrics=['accuracy']) 

# Загрузка датасета MNIST  

mnist = tf.keras.datasets.mnist 

    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), 

(x_train, y_train), (x_test, y_test) = mnist.load_data() 

# Нормализация данных 

x_train, x_test = x_train / 255.0, x_test / 255.0 

x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) 

x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) 

Далее нам нужно определить тип процессора, на котором будут проис-

ходить вычисления. За счет поддержки библиотекой TensorFlow работы с гра-

фическим  процессором,  нам  достаточно  дополнить  код  соответствующими 

функциями обучения с использованием CPU и GPU отдельно. 

Листинг 2 - код  

# Обучение модели на GPU 

with tf.device('/GPU:0'): 

    start_time_gpu = time.time() 

    end_time_gpu = time.time() 

# Обучение модели на СPU 

with tf.device('/CPU:0'): 

    start_time_cpu = time.time() 

    end_time_cpu = time.time() 

    model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test)) 

    model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test)) 

#Сравнение по затраченному времени на Обучение 

print("Time  taken  to  train  the  model  on  GPU:  {:.2f}  seconds".format(end_time_gpu  - 

print("Time  taken  to  train  the  model  on  СPU:  {:.2f}  seconds".format(end_time_gpu  - 

start_time_gpu)) 

start_time_gpu)) 

6.4 Результат 

 

 

 

 

 

 

При обучении модели с использованием GPU, мы можем заметить, что 

параметр  шага  обучения  равен  2ms/step  и  суммарное  время,  затраченное  на 

обучение,  равняется  47.96  секундам.  Процесс  обучения  представлен  на  ри-

сунке 4. 

Рисунок 4 - Процесс обучения на GPU 

При обучении модели с использованием CPU, мы можем заметить, что 

параметр  шага  обучения  равен  7ms/step  и  суммарное  время,  затраченное  на 

обучение,  равняется  136.17  секундам. Процесс  обучения  представлен  на  ри-

 

 

сунке 5. 

 

 

Рисунок 5 - Процесс обучения на CPU 

 

 

 

Стоит отметить, что в момент обучения процент загруженности GPU и 

CPU, был разным. Даже с учетом того, что экспериментальный центральный 

процессор (CPU) находился в состоянии TurboBoost Active и его тактовая ча-

стота была постоянной и равнялась 4.3GHz, его загруженность при обучении 

не была пиковой и составляла от 55-60%, в то время процент загруженности 

видеокарты равнялся 80-85%. Для наглядной демонстрации была реализована 

таблица со средним процентом загруженности, она представлена на рисунке 

6.  Этот факт говорит нам о том, что при равных условиях CPU не может пол-

ностью раскрыть свой потенциал из-за своих архитектурных особенностей, в 

то  время  как  GPU  успешно  распределил  нагрузку  и  использовал  максимум 

своей производительности. 

Рисунок 6 - Средний процент загруженности CPU и GPU 

 

Вывод  

Выполнив данное научное исследование, мы убедились в том, что архи-

тектура графического процессора (GPU) является наиболее производительной 

и эффективной при работе с большими объемами данных в процессе обучения 

нейронных сетей. Данное преимущество обусловлено конструктивной особен-

ностью данной архитектуры и возможностью параллельной обработки боль-

ших объемов данных, в отличие от центральных процессоров, оптимизирован-

ных для последовательной обработки данных. 

 

Список источников 

1.  Гудфеллоу  Я.,  Бенджио  И.,  Курвилль  А.  −  Глубокое  обучение, 

2.  Орельен  Жерон  −  Прикладное  машинное  обучение  с  помощью 

Scikit-Learn и TensorFlow, 2018 г. 

3.  Тарик Рашид – Создаем нейронную сеть, 2016 г. 

4.  Калачев А. – Многоядерные процессоры. Учебное пособие, 2014 

2017 г. 

г. 

5.  Таненбаум Эндрю, Остин Т. – Архитектура компьютера, 2019 г. 

6.  Таненбаум Эндрю, Бос Херберт – Современные операционные си-

стемы, 2019 г. 

